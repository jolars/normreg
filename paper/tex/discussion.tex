\section{Discussion}

% TODO: Consider reducing the content here now that the introduction contains
% a summary of our contributions

This is the first paper to study the effects of normalization in lasso, ridge, and elastic
net regression with binary data. We have discovered that the class balance (proportion of
ones) of these binary features has a pronounced effect on both lasso and ridge estimates
and that this effect depends on the type of normalization used. For the lasso, for
instance, features with large class imbalances stand little chance of being selected if the
binary features are standardized or, to an even greater extent, if they are not scaled at
all---even if their relationships with the response are strong. Not scaling binary features
is a common approach in practice, for instance recommended in the \pkg{scikit-learn}
documentation when the data is sparse~\citep{scikit-learndevelopers2025}. As is
standardization, which is the default in many software packages for the lasso, such as
\pkg{glmnet}~\citep{friedman2010}, as well as the practice taken by countless applications
in research. All in all, this means that the class-balance bias effect is likely pervasive
in practice and risks having already influenced the conclusions drawn in analyses of many
datasets.

The driver of this bias is the relationship between the variance of the feature and type of
normalization. This works as expected for normally distributed features. But for binary
features it means that a one-unit change is treated differently depending on the
corresponding feature's class balance, which we believe may surprise some. We have,
however, shown that scaling binary features with standard deviation in the case of ridge
regression and variance in the case of the lasso mitigates this effect, but that doing so
comes at the price of increased variance. This effectively means that the choice of
normalization constitutes a bias--variance trade-off.

We have also studied the case of mixed data: designs that include both binary and normally
distributed features~(\Cref{sec:mixed-data}). In this setting, our first finding is that
there is an implicit relationship between the choice of normalization and the manner in
which regularization affects binary vis-Ã -vis normally distributed features, even when the
binary feature is perfectly balanced. The choice of max--abs normalization, for instance,
leads to a specific weighting of the effects of binary features relative to those of normal
features.

For interactions between binary and normal features~(\Cref{sec:experiments-interactions}),
our conclusion is that the interaction feature---contrary to what recent literature on
interactions in the lasso recommends---should be scaled with the \emph{product} of the
standard deviation of the normal feature and variance of the binary feature to avoid this
effect of class imbalance. We have not seen this recommendation in the literature before,
but it is a natural extension of our other results.

We note that our theoretical results are limited by a few assumptions: 1) a fixed feature
matrix \(\bm{X}\), 2) normal and independent errors, and 3) orthogonal features. The first
and second of these assumptions are standard in the literature. The third assumption on
orthogonality, however, is strong and rarely satisfied in practice. Yet, as we show in
\Cref{sec:orthogonality-assumption} and our experiments, the assumption does not in fact
appear to be restrictive for our results, which, at least empirically, hold under much more
general settings. We have also focused on the case of binary and continuous features here,
but are convinced that categorical features are also of interest and might raise additional
challenges with respect to normalization. Finally, most of our results are restricted to
least-squares loss, but since all generalized linear models (GLMs) are parameterized by the
linear predictor, which we have shown to depend directly on class balance, we believe that
our results are also relevant for other loss functions. Our initial results in
\Cref{sec:normalization-tuning} seem to support this claim, but we defer further
investigation of this to future work.

Regularized regression models are widely used in practice, and are staples of popular
machine learning and statistical software packages such as
\pkg{glmnet}~\citep{friedman2010}, \pkg{scikit-learn}~\citep{pedregosa2011},
\pkg{mlpack}~\citep{curtin2023}, \pkg{skglm}~\citep{bertrand2022},
\pkg{LIBLINEAR}~\citep{fan2008a}, and \pkg{MATLAB}~\citep{themathworksinc.2022}. Our
results suggest that the choice of normalization is an important aspect of using these
models that, in spite of the popularity of these methods, has so far been overlooked. We
hope that our results will motivate researchers and practitioners to consider the choice of
normalization more carefully in the future.
