\section{Discussion}

% TODO: Consider reducing the content here now that the introduction contains
% a summary of our contributions

This is the first paper to study the effects of normalization in lasso, ridge, and elastic
net regression with binary data. We have discovered that the class balance (proportion of
ones) of these binary features has a pronounced effect on both lasso and ridge estimates
and that this effect depends on the type of normalization used. For the lasso, for
instance, features with large class imbalances stand little chance of being selected if the
features are standardized, even when their relationships with the response are strong.

The driver of this result is the relationship between the variance of the feature and type
of normalization. This works as expected for normally distributed features. But for binary
features it means that a one-unit change is treated differently depending on the
corresponding feature's class balance, which we believe may surprise some. We have,
however, shown that scaling binary features with standard deviation in the case of ridge
regression and variance in the case of the lasso mitigates this effect, but that doing so
comes at the price of increased variance. This effectively means that the choice of
normalization constitutes a bias--variance trade-off.

We have also studied the case of mixed data: designs that include both binary and normally
distributed features~(\Cref{sec:mixed-data}). In this setting, our first finding is that
there is an implicit relationship between the choice of normalization and the manner in
which regularization affects binary viz-a-viz normally distributed features, even when the
binary feature is perfectly balanced. The choice of max--abs normalization, for instance,
leads to a specific weighting of the effects of binary features relative to those of normal
features.

For interactions between binary and normal features
features~(\Cref{sec:experiments-interactions}), our conclusions are that the interaction
feature should be computed after centering both the binary and normal feature and that
scaling with the product of the standard deviation of the normal feature and variance of
the binary features mitigates the class balance bias in this case.

Finally, note that our theoretical results are limited by several assumptions: 1) a fixed
feature matrix \(\mat{X}\), 2) orthogonal features, and 3) normal and independent errors.
In future studies, it would be interesting to relax these assumptions and study the effects
of normalization in a more general setting. We have also focused on the case of binary and
continuous features here, but are convinced that categorical features are also of interest
and might raise additional challenges with respect to normalization.

Regularized regression models are widely used in practice, and are staples of popular
machine learning and statistical software packages such as
\pkg{glmnet}~\citep{friedman2010}, \pkg{scikit-learn}~\citep{pedregosa2011},
\pkg{mlpack}~\citep{curtin2023}, \pkg{skglm}~\citep{bertrand2022},
\pkg{LIBLINEAR}~\citep{fan2008a}, and \pkg{MATLAB}~\citep{themathworksinc.2022}. Our
results suggest that the choice of normalization is an important aspect of using these
models that, in spite of the popularity of these methods, has so far been overlooked. We
hope that our results will motivate researchers and practitioners to consider the choice of
normalization more carefully in the future.
