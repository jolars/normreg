\section{Discussion}\label{sec:discussion}

We have studied the effects of normalization in ridge regression and the lasso for binary
features---an issue that has so far been neglected in the literature. We have discovered
the class balance (proportion of ones and zeros) of these binary features have a pronounced
effect on both lasso and ridge estimates, and that this effect depends on the type of
normalization used. For the lasso, for instance, our results show that features with large
class imbalances stand little chance of being selected even when the their relationships
with the response are strong.

The primary driver of this is the relationship between the variance of the features in the
design and the lasso and ridge estimators. Regardless of the distribution of the feature,
its variance together with the true effect and type of normalization will determine the
amount of shrinkage applied to that feature's respective coefficient. This works naturally
in the case of normally distributed features, since for two features with equivalent
effects but different scales, the true coefficients are also different. But for binary
features, this means that a one-unit change is treated differently among binary features
with different class balances, which has consequences that we believe may surprise users of
these models. It means, for instance, that collecting additional data on a subgroup in the
data set influences the estimates from the model in a specific direction. We have, however,
found that scaling binary features with standard deviation in the case of ridge regression
and variance in the case of the lasso mitigates this effect, but that doing so comes at the
price of increased variance. This effectively means that the choice of normalization
constitutes a bias--variance trade-off.

To study these effects, we have introduced the scaling parameterization $s_j = (q_j -
  q_j^2)^\delta,$ which includes the cases \(\delta=0\) (no scaling, as in max--abs and
min--max normalization), \(\delta = 1/2\) (standard deviation scaling, as in
standardization), and \(\delta=1\) (variance scaling). As far as we know, scaling with
\(\delta=1\) has not been considered in the literature before. Our results demonstrate that
the choice of \(\delta\) affects the lasso and ridge estimates in many cases. This is
especially true with respect to feature selection, in which case \(\delta=0\) will reduce
the chances of finding the true model via the lasso in class-imbalanced
settings~(\Cref{sec:experiments-varbias}). But it also biases the regression coefficients
in both the lasso and ridge, which may lead to suboptimal predictive
performance~(\Cref{sec:predictive-performance}). Both our theoretical
results~(\Cref{sec:theory-binary-features}) and
experiments~(\Cref{sec:experiments-varbias}) show that the optimal choice of \(\delta\) may
depend on the error in the data-generating process, which is typically unknown. As an
alternative, we have investigated choosing \(\delta\) in a data-driven manner by optimizing
over \(\delta\) as if it were a hyperparameter~(\Cref{sec:experiments-hyperparameter}).

We have also studied the case of mixed data: designs that include both binary and normally
distributed features~(\Cref{sec:mixed-data}). In this setting, our first finding is that
there is an implicit relationship between the choice of normalization and the manner in
which regularization affects binary viz-a-viz normally distributed features, even when the
binary feature is perfectly balances. The choice of max--abs normalization, for instance,
carries a specific assumption about how the effect of a binary feature should be compared
to that of a normally distributed feature.

When it comes to the case of simple interactions between binary and normal features
features~(\Cref{sec:experiments-interactions}), our conclusions are that the interaction
feature should be computed after centering both the binary and normal feature and that
scaling with the product of the standard deviation and \((q-q^2)\) mitigates the class
balance bias in this case.

Finally, note that our theoretical results are limited by several assumptions: 1) a fixed
feature matrix \(\mat{X}\), 2) orthogonal features, and 3) normal and independent errors.
In future studies, it would be interesting to relax these assumptions and study the effects
of normalization in a more general setting. Finally, we have also focused on the case of
binary and continuous features here, but we are convinced that categorical features are
also of interest and might raise additional challenges with respect to normalization.
