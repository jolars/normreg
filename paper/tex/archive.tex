% NOTE: In its current form, the following is not useful at all for us, but perhaps it 
% could be if it was rewritten as mse with respec to \beta^*_j instead.

Next, for the mean-squared error with respect to \(\mu\), let \(\risk(\lambda, \mu) = \E \big(\st_\lambda(Z) - \mu\big)^2\) for brevity. Now observe that\todojl{Need to rewrite this using \(\beta^*_j\) instead.}
\[
  \risk(\lambda, \mu) = \int_{-\infty}^\frac{-\lambda - \mu}{\sigma} (\sigma u + \lambda)^2 \pdf(u)\du u +
  \int_{\frac{\lambda - \mu}{\sigma}}^\infty (\sigma u - \lambda)^2 \pdf(u)\du u + \mu^2 \int_{\frac{-\lambda - \mu}{\sigma}}^{\frac{\lambda - \mu}{\sigma}} \pdf(u)\du u.
\]
Expanding, we have
\[
  \begin{aligned}
    R(\lambda,\mu) & = \sigma^2 \int_{-\infty}^\frac{-\lambda - \mu}{\sigma} u^2 \pdf(u)\du u + \sigma^2 \int_\frac{\lambda - \mu}{\sigma}^\infty u^2 \pdf(u)\du u + 2\lambda \int_{-\infty}^\frac{-\lambda - \mu}{\sigma} u \pdf(u)\du u \\
                   & \phantom{=} - 2\lambda \int_\frac{\lambda - \mu}{\sigma}^{\infty} u \pdf(u)\du u + \lambda^2 \int_{-\infty}^\frac{-\lambda-\mu}{\sigma} \pdf(u)\du u + \lambda^2 \int_\frac{\lambda-\mu}{\sigma}^\infty \pdf(u)\du u \\
                   & \phantom{=} + \mu^2 \int_\frac{-\lambda - \mu}{\sigma}^\frac{\lambda - \mu}{\sigma} \pdf(u) \du u.
  \end{aligned}
\]

Next, assume that \(\mu \geq 0\). Now we compute an upper bound for the mean-squared error. First, observe that
\begin{equation}
  \label{eq:mse-bound1}
  \lim_{\mu \rightarrow \infty} \risk(\lambda, \mu) = \sigma^2 + \lambda^2.
\end{equation}
% We need some condition on the last integral for this to hold, since mu^2 may also get lar0/000ge.
Next, we see that
\begin{equation}
  \label{eq:mse-bound2}
  \risk(\lambda, \mu) - \risk(\lambda, 0) = -2\mu \E \st_\lambda(z) + \mu^2 \leq \mu^2
\end{equation}
under the assumption that \(\mu \geq 0\). Using \eqref{eq:mse-bound1} and \eqref{eq:mse-bound2}, we have
\[
  \risk(\lambda,\mu) \leq \min\big(\sigma^2 + \lambda^2, \risk(\lambda, 0) + \mu^2\big) \leq \risk(\lambda, 0) + \min(\mu^2, \sigma^2 + \lambda^2).
\]


% Old proof for expected value for class-imbalanced case, but only for \delta = 1
\begin{proof}
  OLD PROOF for expected value case.

  First note that when \(s = q(1-q)\) we have
  \[
    \mu = \beta_j^* n, \quad \sigma^2 = \frac{\sigma_\varepsilon^2 n}{q(1-q)}, \quad\text{and}\quad d_j = n
  \]
  so that
  \[
    \begin{aligned}
      \theta                & = -\beta_j^* n - \lambda,                                                    \\
      \gamma                & = \beta_j^* n - \lambda                                                      \\
      \frac{\theta}{\sigma} & = \frac{-\sqrt{q(1-q)}(\beta_j^* n + \lambda)}{\sigma_\varepsilon \sqrt{n}}, \\
      \frac{\gamma}{\sigma} & = \frac{\sqrt{q(1-q)}(\beta_j^*n - \lambda)}{\sigma_\varepsilon \sqrt{n}}.
    \end{aligned}
  \]
  Let
  \[
    a = \frac{\beta_j^* n + \lambda}{\sigma_\varepsilon \sqrt{n}}, \quad\text{and}\quad b = \frac{\beta_j^* n - \lambda}{\sigma_\varepsilon \sqrt{n}}.
  \]
  We now have
  \[
    \lim_{q \rightarrow 1} -\theta \cdf\left(\frac{\theta}{\sigma}\right) = (\beta_j^*n + \lambda) \lim_{q \rightarrow 1} \cdf\left(-a\sqrt{q(1-q)}\right) = \frac{1}{2}(\beta_j^*n + \lambda)
  \]
  and similarly
  \[
    \lim_{q \rightarrow 1} \gamma \cdf\left(\frac{\gamma}{\sigma}\right) = \frac{1}{2}(\beta_j^*n - \lambda)
  \]
  since \(\sqrt{q(1-q)} \rightarrow 0\) and hence \(\cdf\big(-a\sqrt{q(1-q)}\big) \rightarrow 1/2\) as \(q \rightarrow 1\), reminding ourselves
  that \(\cdf\) is the cumulative distribution function of the standard normal distribution, which is symmetric around 0, and that \(a\) is constant with respect to \(q\).

  Next, we define
  \[
    f(q) = \sigma_\varepsilon \sqrt{n}\left(\pdf\left(b\sqrt{q(1-q)}\right) - \pdf\left(-a\sqrt{q(1-q)}\right)\right),\quad\text{and}\quad g(q) = \sqrt{q(1-q)}
  \]
  and take the derivatives thereof, yielding
  \[
    f'(q) = \frac{\sigma_\varepsilon \sqrt{n} (2q -1)}{4 \pi}\left(b^2 e^{-\frac{q(1-q)b^2}{2}}- a^2 e^{-\frac{q(1-q)a^2}{2}}\right)
  \]
  and
  \[
    g'(q) = \frac{1 - 2q}{2\sqrt{q(1-q)}}.
  \]
  Note that
  \[
    \lim_{q \rightarrow 1} f(q) = \lim_{q \rightarrow 1} g(q) = 0
  \]
  and moreover that \(f(q)\) and \(g(q)\) are both differentiable in the interval \((1/2, 1)\) and \(g'(q) \neq 0\) everywhere in this interval.

  Finally, note that
  \[
    \begin{aligned}
      \frac{f'(q)}{g'(q)} & = \frac{2\sqrt{q(1-q)}\sigma_\varepsilon \sqrt{n} (2q  - 1)\left(b^2 e^{-\frac{q(1-q)b^2}{2}}- a^2 e^{-\frac{q(1-q)a^2}{2}}\right)}{1 - 2q} \\
                          & =2\sqrt{q(1-q)}\sigma_\varepsilon \sqrt{n} \left(a^2 e^{-\frac{q(1-q)a^2}{2}}- b^2 e^{-\frac{q(1-q)b^2}{2}}\right)                          \\
                          & \rightarrow 0 \text{ as } q \rightarrow 1.
    \end{aligned}
  \]
  Since this limit exists and is finite, we may apply L'HÃ´pital's rule to obtain
  \[
    \lim_{q \rightarrow 1} \frac{f(q)}{g(q)} = \lim_{q \rightarrow 1} \frac{f'(q)}{g'(q)} = 0.
  \]
  Finally, we piece together our previous efforts to obtain
  \[
    \begin{aligned}
      \lim_{q \rightarrow 1} \hat\beta_j & = \frac{1}{d} \lim_{q \rightarrow 1}\left(- \theta \cdf\left(\frac{\theta}{\sigma}\right) - \sigma \pdf\left(\frac{\theta}{\sigma}\right) + \gamma \cdf\left(\frac{\gamma}{\sigma}\right) + \sigma \pdf\left(\frac{\gamma}{\sigma}\right)\right) \\
                                         & = \frac{1}{n}\left(\beta_j^*n +  \lim_{q \rightarrow 1}\frac{f(q)}{f(q)} \right)                                                                                                                                                                 \\
                                         & = \beta_j^*.
    \end{aligned}
  \]
\end{proof}

% TODO: We might want to rephrase this somehow.
% 
% \begin{corollary}
%   Suppose \(\sigma_\varepsilon = 0\) in \Cref{thm:binary-varscale-bias}, all other things being equal. Then we instead obtain
%   \[
%     \lim_{q \rightarrow 1} \E \hat{\beta}_j = \beta^*_j - \sign(\beta^*_j)\frac{\lambda}{n},
%   \]
%   which is the typical result of the lasso estimate when \(\vec{x}_j\) is normally distributed.
% \end{corollary}

