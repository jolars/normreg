\section{Normalization}

There is considerable ambiguity regarding key terms in the literature. Here, we define
\emph{normalization} as the process of centering and scaling the feature matrix, which we
now formalize.

% \begin{definition}[Normalization]
%   Let \(\tilde{\mat X}\) be the normalized feature matrix with elements
%   \(\tilde{x}_{ij} = (x_{ij} - c_{j})/s_j\), where \(x_{ij}\) is an element of
%   \(\mat{X}\) and \(c_j\) and \(s_j\) are the \emph{centering} and
%   \emph{scaling} factors respectively.
% \end{definition}

\begin{definition}[Normalization]
  \label{def:normalization}
  Let \(\mat{X} \in \mathbb{R}^{n\times p}\) be the feature matrix and let
  \(\vec{c} \in \mathbb{R}^p\) and \(\vec{s} \in \mathbb{R}^p_+\) be centering
  and scaling factors respectively. Then \(\tilde{\mat{X}}\) is the
  \emph{normalized} feature matrix with elements given by
  \(\tilde{x}_{ij} = (x_{ij} - c_j)/s_j\).
\end{definition}

% Some authors refer to the procedure in \Cref{def:normalization} as \emph{standardization},
% but here we define standardization only as the case when centering with the mean and
% scaling with the (uncorrected) standard deviation.

There are many different normalization strategies and we have listed a few common choices
in \Cref{tab:normalization-types}. Standardization is perhaps the most popular type of
normalization, at least in the field of statistics. One of its benefits is that it
simplifies certain aspects of fitting the model, such as fitting the intercept. The
downside of standardization is that it involves centering by the mean, which destroys
sparsity.
% This is not a problem when \(\bm{X}\) is stored as a dense matrix; but when the
% data is sparse, it may increase memory use and processing time.
%
\begin{table}[hbt]
  \centering
  \begin{tabular}{lll}
    \toprule
    a & b & c \\
    \midrule
    1 & 2 & 3 \\
    \bottomrule
  \end{tabular}
  \caption{
    A caption
  }
\end{table}

\begin{table}[hbt]
  \centering
  \caption{
    Common ways to normalize a matrix of features using centering and scaling
    factors \(c_j\) and \(s_j\), respectively. Note that \(\bar{x}_j\) is
    the arithmetic mean of feature \(j\).
  }
  \label{tab:normalization-types}
  \begin{tabular}{lll}
    \toprule
    Normalization            & \(c_{j}\)          & \(s_j\)                                                   \\
    \midrule
    Standardization          & \(\bar{x}_j\)      & \(\sqrt{\frac{1}{n}\sum_{i=1}^n (x_{ij} - \bar{x}_j)^2}\) \\
    \addlinespace
    Max--Abs                 & 0                  & \(\max_i|x_{ij}|\)                                        \\
    \addlinespace
    Min--Max                 & \(\min_i(x_{ij})\) & \(\max_i(x_{ij}) - \min_i(x_{ij})\)                       \\
    \addlinespace
    \(\ell_1\)-Normalization & 0 or \(\bar{x}_j\) & \(\lVert \vec{x}_j\rVert_1\)                              \\
    % \addlinespace
    % Adaptive Lasso           & 0                  & \(\hat{\beta}_j^\text{OLS}\)                              \\
    \bottomrule
  \end{tabular}
\end{table}

When \(\bm{X}\) is sparse, a common alternative to standardization is min--max or max--abs
(maximum absolute value) normalization, which scale the data to lie in \([0, 1]\) and
\([-1, 1]\) respectively, and retain sparsity when features are binary. These methods are,
however, both sensitive to outliers. And since sample extreme values often depend on sample
size, as in the case of normal data~(\Cref{sec:maxabs-theory}), use of these methods may
sometimes be problematic.
% A special case of normalization is the adaptive
% lasso~\citep{zou2006}, in which the coefficients from an initial model fit (typically OLS
% or ridge regression) are used as scaling factors.

