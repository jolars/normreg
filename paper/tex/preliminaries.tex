\section{Preliminaries}

Throughout the paper we assume that the response \(\vec{y}\) is generated according to
\(y_i = \beta_0^* + \vec x_i^\T \vec\beta^* + \varepsilon_i\) for \(i \in [n]\) where \([n]
= \{1,2,\dots,n\}\). \(\mat X\) is the \(n \times p\) design matrix with features \(\vec
x_j\). We assume \(\mat{X}\), \(\beta_0^*\), and \(\vec{\beta}^*\) to be fixed.

There is ambiguity regarding key terms in the literature. Here, we define
\emph{normalization} as the process of centering and scaling the feature matrix, which we
now formalize.

\begin{definition}[Normalization]
  \label{def:normalization}
  Let \(\tilde{\mat X}\) be the normalized feature matrix with elements
  \(\tilde{x}_{ij} = (x_{ij} - c_{j})/s_j\), where \(x_{ij}\) is an element of
  \(\mat{X}\) and \(c_j\) and \(s_j\) are the \emph{centering} and
  \emph{scaling} factors respectively.
\end{definition}

% Some authors refer to the procedure in \Cref{def:normalization} as \emph{standardization},
% but here we define standardization only as the case when centering with the mean and
% scaling with the (uncorrected) standard deviation.

\subsection{Normalization Strategies}

There are many different normalization strategies and we have listed a few common choices
in \Cref{tab:normalization-types}. Standardization is perhaps the most popular type of
normalization, at least in the field of statistics. One of its benefits is that it
simplifies certain aspects of fitting the model, such as fitting the intercept. The
downside of standardization is that it involves centering by the mean, which destroys
sparsity.
% This is not a problem when \(\bm{X}\) is stored as a dense matrix; but when the
% data is sparse, it may increase memory use and processing time.

% TODO: move to appendix?
\begin{table}[hbt]
  \centering
  \caption{
    Common ways to normalize a matrix of features using centering and scaling
    factors \(c_j\) and \(s_j\), respectively. Note that \(\bar{x}_j\) is
    the arithmetic mean of feature \(j\).
  }
  \label{tab:normalization-types}
  \vskip 0.15in
  \small
  \begin{tabular}{lll}
    \toprule
    Normalization            & \(c_{j}\)          & \(s_j\)                                                   \\
    \midrule
    Standardization          & \(\bar{x}_j\)      & \(\sqrt{\frac{1}{n}\sum_{i=1}^n (x_{ij} - \bar{x}_j)^2}\) \\
    \addlinespace
    Max--Abs                 & 0                  & \(\max_i|x_{ij}|\)                                        \\
    \addlinespace
    Min--Max                 & \(\min_i(x_{ij})\) & \(\max_i(x_{ij}) - \min_i(x_{ij})\)                       \\
    \addlinespace
    \(\ell_1\)-Normalization & 0 or \(\bar{x}_j\) & \(\lVert \vec{x}_j\rVert_1\)                              \\
    % \addlinespace
    % Adaptive Lasso           & 0                  & \(\hat{\beta}_j^\text{OLS}\)                              \\
    \bottomrule
  \end{tabular}
\end{table}

When \(\bm{X}\) is sparse, a common alternative to standardization is to use either
min--max or max--abs (maximum absolute value) normalization, which scales the data to lie
in respectively \([0, 1]\) or \([-1, 1]\) and retains sparsity when the features are
binary. These methods are, however, both sensitive to outliers. For many types of data,
such as normally distributed data, it is also often the case that the sample maximum
depends on sample size, which often makes use of min--max and max--abs normalization
problematic~(\Cref{thm:maxabs-gev}).
% A special case of normalization is the adaptive
% lasso~\citep{zou2006}, in which the coefficients from an initial model fit (typically OLS
% or ridge regression) are used as scaling factors.

\subsection{Lasso, Ridge, and Elastic Net Regression}%
\label{sec:elastic-net-solution}

If we include an intercept and assume that the features of the normalized design matrix are
orthogonal, that is, \(\tilde{\mat{X}}^\intercal \tilde{\mat{X}} =
\diag\left(\tilde{\vec{x}}_1^\T \tilde{\vec{x}}_1, \dots, \tilde{\vec{x}}_p^\intercal
\tilde{\vec{x}}_p\right) \), the solution to the coefficients in the elastic net problem is
given by
%
\begin{equation}
  \label{eq:orthogonal-solution-normalized}
  \hat{\beta}^{(n)}_j = \frac{\st_{\lambda_1}\left(\tilde{\vec{x}}_j^\T \vec{y}\right)}{\tilde{\vec{x}}_j^\T \tilde{\vec{x}}_j + \lambda_2},
  \qquad
  \hat{\beta}_0^{(n)} = \frac{\vec{y}^\T \ones}{n},
\end{equation}
%
where \(\st_\lambda(z) = \sign(z) \max(|z| - \lambda, 0)\) is the soft-thresholding
operator. (See \Cref{sec:elastic-net-estimator} for a derivation of this.)

Normalization changes the optimization problem and its solution, the coefficients, which
will now be on the scale of the normalized features. But we are interested in
\(\hat{\vec{\beta}}\): the coefficients on the scale of the original problem. To obtain
these, we transform the coefficients from the normalized poblem, \(\hat\beta^{(n)}_j\),
back via \(\hat\beta_j = \hat\beta^{(n)}_j/s_j\) for \(j \in [p]\). There is a similar
transformation for the intercept but we omit here since we are not interested in it.
