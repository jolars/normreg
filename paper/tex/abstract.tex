Regularized models are often sensitive to the scales of the features in the data and it has
therefore become standard practice to normalize (center and scale) features before fitting
the model. But there are many different ways to normalize the features and the choice may
have dramatic effects on the resulting model. In this paper, we study binary and normally
distributed features in the context of lasso, ridge, and elastic net regression. We show
that the class balances of binary features directly influences the regression coefficients
and that this effect depends on the combination of normalization strategy and
regularization method (lasso, ridge, or elastic net) used. We moreover show that this bias
can be mitigated by scaling binary features with their variance in the case of the lasso
and standard deviation in the case of ridge regression, but that this comes at the cost of
increased variance. For the elastic net, we demonstrate that scaling the penalty weights,
rather than the features, can achieve the same effect. Finally, we also tackle the case of
mixes of binary and normal features as well as interactions and provide some intial results
on how to normalize features in these cases.
