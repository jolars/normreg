%! TeX program = xelatex
\PassOptionsToPackage{unicode,pdfusetitle}{hyperref}
\PassOptionsToPackage{hyphens}{url}
\PassOptionsToPackage{dvipsnames,svgnames,x11names}{xcolor}
\PassOptionsToPackage{math-style=ISO,bold-style=ISO}{unicode-math}

\documentclass[10pt,onlytextwidth]{beamer}

\newcommand{\pkg}[1]{\textsf{#1}}
\newcommand{\data}[1]{\texttt{#1}}

\usetheme{moloch}
\usefonttheme{professionalfonts}
\setbeamertemplate{page number in head/foot}[appendixframenumber]

\usepackage{amssymb,amsmath,mathtools,amsthm}
% \usepackage[T1]{fontenc}
\usepackage{textcomp}

\usepackage{upquote} % straight quotes in verbatim environments
\usepackage{microtype}
\UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts

\usepackage{fontsetup}

\usepackage{xcolor}
\usepackage{xurl} % add URL line breaks if available
\usepackage{bookmark}
\usepackage{hyperref}

\hypersetup{%
  colorlinks = true,
  linkcolor  = mLightGreen,
  filecolor  = mLightGreen,
  citecolor  = mLightGreen,
  urlcolor   = mLightGreen
}

%% subfigures
\usepackage{caption}
\usepackage{subcaption}

\usepackage{booktabs}

\usepackage{siunitx}

% bibliography
\usepackage[style=authoryear]{biblatex}
\addbibresource{bibliography.bib}

% title block
\titlegraphic{\hfill\includegraphics[width=1.8cm]{figures/logo.pdf}}
\title{The Choice of Normalization Influences Shrinkage in Regularized Regression}
\subtitle{TMLR 2025}
\author{\textbf{Johan Larsson} \and Jonas Wallin\\\smallskip\scriptsize \url{https://jolars.co}, {\texttt{@jolars@mastodon.social}}}
\institute{Department of Mathematical Sciences, Copenhagen University}
\date{November 11, 2025}

\input{tex/macros.tex}

\begin{document}

\maketitle

\begin{frame}[c]
  \frametitle{The Elastic Net}

  Linear regression plus a combination of the \(\ell_1\) and \(\ell_2\) penalties:
  \begin{equation*}
    \hat{\vec{\beta}} = \operatorname*{arg\,min}_{\beta \in \mathbb{R}^p} \bigg( \frac{1}{2} \lVert \vec y - \mat{X}\vec{\beta} \rVert^2_2  + \underbrace{\lambda_1 \lVert \vec\beta \rVert_1}_\text{lasso} + \underbrace{\frac{\lambda_2}{2}\lVert \vec \beta \rVert_2^2}_\text{ridge}\bigg)
  \end{equation*}

  \pause

  \begin{figure}
    \centering
    \includegraphics[width=\textwidth]{figures/elasticnet-balls.pdf}
    \caption{%
      Elastic net is a combination of the lasso and ridge.
    }
  \end{figure}
\end{frame}

% TODO: add a simple demonstration using linear regression of this

\begin{frame}[c]
  \frametitle{The Elastic Net Path}

  \begin{columns}
    \begin{column}{0.45\textwidth}
      We don't know optimal \(\lambda_1\) and \(\lambda_2\) in advance;
      instead we use hyper-parameter optimization (e.g. cross-validation).

      \medskip\pause

      Common parametrization:
      \begin{align*}
        \lambda_1 & = \alpha\lambda,     \\
        \lambda_2 & = (1- \alpha)\lambda
      \end{align*}
      with \(\alpha \in [0, 1]\).

      \medskip\pause

      For each \(\alpha\), solve the elastic net over a sequence of \(\lambda\): the
      \textbf{elastic net path}.
    \end{column}
    \begin{column}{0.5\textwidth}
      \only<4->{%
        \begin{figure}[htpb]
          \centering
          \includegraphics[width=\textwidth]{figures/elasticnet_path.pdf}
          \caption{%
            The elastic net path
          }
        \end{figure}
      }
    \end{column}
  \end{columns}

\end{frame}

\begin{frame}[c]
  \frametitle{Sensitivity to Scale}

  Lasso and ridge penalties are \textbf{norms}---feature scales matter!

  \pause

  \begin{exampleblock}{Example}
    Assume
    \[
      \mat{X} \sim \operatorname{Normal}\left(\begin{bmatrix}0 \\ 0\end{bmatrix}, \begin{bmatrix} 4 & 0 \\ 0 & 1\end{bmatrix}\right), \qquad \vec{\beta}^* = \begin{bmatrix} \frac{1}{2} \\ 1 \end{bmatrix}
    \]
    and set \(\symbf{y} = \symbf{X}\symbf{\beta} + \varepsilon\), with
    \(\varepsilon \sim \operatorname{Normal}(0, \sigma_\varepsilon^2)\).

    \medskip\pause

    \begin{table}
      \begin{tabular}{lcc}
        \toprule
        Model & \(\hat{\symbf{\beta}}^{(n)}\)                          & \(\hat{\vec{\beta}}_\text{std}\)                             \\
        \midrule
        OLS   & \(\begin{bmatrix} 0.50 & 1.00\end{bmatrix}^\intercal\) & \(\begin{bmatrix}1.00 & 1.00\end{bmatrix}^\intercal\) \pause \\
        Lasso & \(\begin{bmatrix} 0.38 & 0.50\end{bmatrix}^\intercal\) & \(\begin{bmatrix}0.74 & 0.50\end{bmatrix}^\intercal\) \pause \\
        Ridge & \(\begin{bmatrix} 0.37 & 0.41\end{bmatrix}^\intercal\) & \(\begin{bmatrix}0.74 & 0.41\end{bmatrix}^\intercal\)        \\
        \bottomrule
      \end{tabular}
    \end{table}
  \end{exampleblock}

  \pause

  \alert{Large} scale means \alert{less} penalization because the size of
  \(\beta_j\) can be smaller for an equivalent effect (on \(\vec{y}\)).

\end{frame}

\begin{frame}[c]
  \frametitle{Normalization}

  Scale sensitivity can be mitigated by normalizing the features. Let \(\tilde{\mat X}\) be
  the normalized feature matrix, with elements
  \[
    \tilde{x}_{ij} = \frac{x_{ij} - c_{j}}{s_j}.
  \]

  \medskip\pause

  After fitting, we transform the coefficients back to their original scale via
  \[
    \hat\beta_j = \frac{\hat\beta^{(n)}_j}{s_j} \quad\text{for}\quad j = 1,2,\dots,p,
  \]
  where \(\hat\beta^{(n)}_j\) is a coefficient from the normalized problem.
\end{frame}

\begin{frame}[c]
  \begin{table}[hbt]
    \centering
    \caption{Common ways to normalize \(\mat{X}\)}
    \begin{tabular}{lll}
      \toprule
      Normalization            & \(c_{j}\)          & \(s_j\)                                                     \\
      \midrule
      Standardization          & \(\bar{x}_j\)      & \(\frac{1}{\sqrt{n}} \lVert \vec{x}_j - \bar{x}_j\rVert_2\) \\
      \addlinespace
      \(\ell_1\)-Normalization & \(\bar{x}_j\)      & \(\frac{1}{\sqrt{n}} \lVert \vec{x}_j - \bar{x}_j\rVert_1\) \\
      \addlinespace
      Max--Abs                 & 0                  & \(\max_i|x_{ij}|\)                                          \\
      \addlinespace
      Min--Max                 & \(\min_i(x_{ij})\) & \(\max_i(x_{ij}) - \min_i(x_{ij})\)                         \\
      \bottomrule
    \end{tabular}
  \end{table}
\end{frame}

\begin{frame}[c]
  \begin{figure}[htpb]
    \centering
    \includegraphics[width=0.65\textwidth]{figures/realdata_paths_small.pdf}
    \caption{%
      Normalization matters. Lasso paths under two different types of normalization (standardization and max--abs normalization). The union of the first five features selected in any of the settings are colored.
    }
  \end{figure}

\end{frame}

\begin{frame}[c]
  \begin{table}[htpb]
    \centering
    \caption{
      Lasso coefficients on test sets, with \(\lambda\) set from 5-fold
      cross-validation repeated 5 times. We show the five largest coefficients
      in magnitude for each data set for the standardization setting and
      corresponding coefficients for the max--abs normalization setting.
    }
    \begin{tabular}{
        S[table-format=-1.2,round-mode=figures,round-precision=2]
        S[table-format=-1.2,round-mode=figures,round-precision=2]
        S[table-format=1.3,round-mode=figures,round-precision=2]
        S[table-format=1.1,round-mode=figures,round-precision=2]
        S[table-format=1.1,round-mode=figures,round-precision=2]
        S[table-format=1.2,round-mode=figures,round-precision=2]
      }
      \toprule
      \multicolumn{2}{c}{\data{housing}}   & \multicolumn{2}{c}{\data{triazines}}      & \multicolumn{2}{c}{\data{w1a}}                                                                                                                                      \\
      \cmidrule(rl){1-2}
      \cmidrule(rl){3-4}
      \cmidrule(rl){5-6}
      {\(\hat{\symbf{\beta}}_\text{std}\)} & {\(\hat{\symbf{\beta}}_\text{max--abs}\)} & {\(\hat{\symbf{\beta}}_\text{std}\)} & {\(\hat{\symbf{\beta}}_\text{max--abs}\)} & {\(\hat{\symbf{\beta}}_\text{std}\)} & {\(\hat{\symbf{\beta}}_\text{max--abs}\)} \\
      \midrule
      -0.6309                              & -0.675004                                 & 0.17369                              & 0.0                                       & 1.80033                              & 0.0                                       \\
      -1.38393                             & -0.779929                                 & 0.0691224                            & 0.0                                       & 1.78789                              & 0.78455                                   \\
      0.265659                             & 0.0                                       & 0.0284156                            & 0.0                                       & 1.78348                              & 0.630671                                  \\
      -0.987113                            & -0.335828                                 & 0.0706408                            & 0.0                                       & 1.44741                              & 0.07977                                   \\
      2.77031                              & 3.06118                                   & 0.0293775                            & 0.0                                       & 1.65553                              & 0.0                                       \\
      \bottomrule
    \end{tabular}
  \end{table}

\end{frame}

\begin{frame}[c]
  \frametitle{Paper Summary}

  \begin{columns}[T]
    \begin{column}{0.45\textwidth}
      \begin{block}{Motivation}

        Normalization matters but there is no research into this.

        \medskip\pause

        Everyone agrees you need to normalize, but how to do so is usually motivated by being
        ``standard''.

        \medskip\pause

        The meaning of ``standard'' depends on field!
      \end{block}
    \end{column}

    \pause

    \begin{column}{0.45\textwidth}

      \begin{block}{What We Show}
        Normalization influences shrinkage for the elastic net
        for binary features.

        \medskip\pause

        There's a bias--variance tradeoff that depends on the normalization.

        \medskip\pause

        For the elastic net, you should scale the penalty weights---not the features.

        \medskip\pause

        When mixing binary and normal features, normalization implicitly weighs their importance.
      \end{block}

    \end{column}
  \end{columns}

\end{frame}

\section{A Little Theory}

\begin{frame}[c]
  \frametitle{Orthogonal Features}

  There is no explicit solution to the elastic net problem in general.

  \pause\bigskip

  \begin{columns}
    \begin{column}{0.5\textwidth}
      But if we assume that the features are orthogonal, that is
      \[
        \tilde{\mat{X}}^\intercal \tilde{\mat{X}} = \diag(\tilde{\vec{x}}_1^\intercal \tilde{\vec{x}}_1, \dots, \tilde{\vec{x}}_p^\intercal \tilde{\vec{x}}_p),
      \]
      then there is:\footnote[frame]{We have also assumed that the features are mean-centered
        here.}:
      \begin{equation*}
        \hat\beta_j = \frac{\st_{\lambda_1}\left(\tilde{\vec{x}}_j^\T \vec{y}\right)}{s_j\left(\tilde{\vec{x}}_j^\T \tilde{\vec{x}}_j + \lambda_2\right)},
      \end{equation*}
      where
      \[
        \st_\lambda(z) = \sign(z) \max(|z| - \lambda, 0).
      \]\pause
    \end{column}
    \begin{column}{0.45\textwidth}
      \begin{figure}[htpb]
        \centering
        \includegraphics[width=\textwidth]{figures/soft-thresholding.pdf}
        \caption{%
          Soft thresholding
        }
      \end{figure}
    \end{column}
  \end{columns}

\end{frame}

\begin{frame}[c]
  \frametitle{Binary Features}
  Assume we have a binary feature \(\vec{x}_j\), such that \(x_{ij} \in \{0, 1\}\). Let
  \(q \in [0, 1]\) be the class balance of this feature, that is: \(q = \bar{\vec{x}}_j\).

  \pause

  \begin{block}{Noiseless Case}
    In the noiseless case, we have
    \[
      \hat{\beta}_j = \frac{\operatorname{S}_{\lambda_1}\left(\frac{\beta_j^* n \alert{(q - q^2)}}{s_j}\right)}{s_j\left(\frac{n\alert{(q - q^2)}}{s_j^2} + \lambda_2\right)}.
    \]
    \pause
    Means that the elastic net estimator depends on class balance (\(q\)).

    \medskip\pause

    \(s_j = q - q^2\) for lasso and \(s_j = \sqrt{q-q^2}\) for ridge removes effect of \(q\),
    which suggests the parametrization
    \[
      s_j = (q - q^2)^\delta, \qquad \delta \geq 0.
    \]

    \medskip\pause

    There is no (simple) \(s_j\) that will work for the elastic net.
  \end{block}
\end{frame}

\begin{frame}[c]
  \frametitle{Probability of Selection}

  Since \(\mat{X}\) is fixed and \(\vec{\varepsilon}\) is normal, we can compute the
  probability of selection.

  \begin{figure}[htpb]
    \centering
    \includegraphics[width=\textwidth]{figures/selection_probability.pdf}
    \caption{%
      Probability that the elastic net selects a feature across different noise levels \((\sigma_\varepsilon)\), types of normalization (\(\delta\)), and class balance (\(q\)).
      The dashed line is asymptotic behavior for \(\delta = 1/2\).
      Scaling used is \(s_j \propto (q - q^2)^\delta\).
    }
  \end{figure}
\end{frame}

\begin{frame}[c]
  % \frametitle{A Bias--Variance Tradeoff}

  \begin{figure}
    \centering
    \includegraphics[width=\textwidth]{figures/bias-var-onedim.pdf}
    \caption{%
      A bias variance tradeoff. Bias, variance, and mean-squared error for a one-dimensional lasso problem. Theoretical result for orthogonal features. Dotted line is asymptotic result or \(\delta = 1/2\).
      Scaling used is \(s_j \propto (q - q^2)^\delta\).
    }
  \end{figure}

\end{frame}

\section{Experiments}

\begin{frame}[c]
  \frametitle{Binary Features (Decreasing \(q\))}

  \begin{figure}[htpb]
    \centering
    % \includegraphics[width=0.86\textwidth]{figures/binary_decreasing.pdf}
    \includegraphics[width=\textwidth]{figures/binary_decreasing.pdf}
    \caption{%
      Lasso estimates for first 30 coefficients in an example where \(n = 500\) and \(p = 1000\). The first 20 features are true signals with a geometrically decreasing class balance from 0.5 to 0.99. \(\rho\) is
      a measure of autocorrelation.
    }
  \end{figure}
\end{frame}

\begin{frame}[c]
  \frametitle{Mixed Data}

  \begin{figure}[htpb]
    \centering
    \includegraphics[width=\textwidth]{figures/mixed-data.pdf}
    \caption{%
      Comparison between different normalization strategies on a two-feature problem with one binary and one normal feature.
      The normal feature is always standardized, while the binary feature is scaled with \(s_j \propto (q - q^2)^\delta\).
    }
  \end{figure}
\end{frame}

\begin{frame}[c]
  \frametitle{Hyperparameter Optimization}

  \begin{figure}[htpb]
    \centering
    \includegraphics[width=\textwidth]{figures/hyperopt_surfaces.pdf}
    \caption{%
      Contour plots of hold-out (validation set) error across a grid of \(\delta\) and \(\lambda\) values for the
      lasso and ridge.
    }
  \end{figure}
\end{frame}

\begin{frame}[c]
  \frametitle{Summary}
  Class balance plays a crucial role when using regularized regression on binary data.

  \medskip\pause

  This is the first paper to investigate the interplay between normalization and
  regularization.

  \medskip\pause

  We introduced a new scaling approach to deal with class-imbalanced binary features.

  \medskip\pause

  \begin{block}{More Details in Paper}

    \begin{itemize}[<+->]
      \item Mixed data
      \item Interactions
      \item The Weighted Elastic Net
      \item Many more experiments on real and simulated data
    \end{itemize}
  \end{block}
\end{frame}

\begin{frame}[standout]
  Thank you!
\end{frame}

\end{document}

